{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1faf6ed",
   "metadata": {},
   "source": [
    "# Pre-trained\n",
    "\n",
    "spaCy dan Stanza adalah library NLP modern yang menyediakan berbagai model siap pakai (pre-trained) untuk berbagai tugas, termasuk POS Tagging. Anggap saja mereka adalah \"tim ahli NLP\" yang sudah dilatih pada miliaran kata dan bisa kamu panggil kapan saja untuk menyelesaikan tugas dengan cepat dan dengan akurasi yang sangat tinggi. Mereka sering digunakan di industri dan akademik sebagai standar pembanding (benchmark)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b929f44c",
   "metadata": {},
   "source": [
    "**Instalasi spaCy**\n",
    "\n",
    "Unduh model bahasa Inggris yang sudah dilatih. Model `en_core_web_sm` adalah model kecil yang efisien\n",
    "\n",
    "```bash\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "Model spaCy lain bisa dilihat di https://spacy.io/models/en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cc729a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Hasil dari spaCy ---\n",
      "'Learning' -> Universal: VERB, Detail: VBG\n",
      "'POS' -> Universal: PROPN, Detail: NNP\n",
      "'Tagging' -> Universal: PROPN, Detail: NNP\n",
      "'is' -> Universal: AUX, Detail: VBZ\n",
      "'challenging' -> Universal: VERB, Detail: VBG\n",
      "'for' -> Universal: ADP, Detail: IN\n",
      "'a' -> Universal: DET, Detail: DT\n",
      "'reason' -> Universal: NOUN, Detail: NN\n",
      "'.' -> Universal: PUNCT, Detail: .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Muat model bahasa Inggris yang sudah diunduh\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "teks = \"Learning POS Tagging is challenging for a reason.\"\n",
    "\n",
    "# Proses teks dengan model spaCy\n",
    "doc = nlp(teks)\n",
    "\n",
    "print(\"--- Hasil dari spaCy ---\")\n",
    "# Iterasi setiap token dalam dokumen yang sudah diproses\n",
    "# .pos_ -> Tag Universal (simpel)\n",
    "# .tag_ -> Tag detail (spesifik)\n",
    "for token in doc:\n",
    "    print(f\"'{token.text}' -> Universal: {token.pos_}, Detail: {token.tag_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5af30a",
   "metadata": {},
   "source": [
    "**Instalasi stanza**\n",
    "\n",
    "```bash\n",
    "pip install stanza\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00de6c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 432kB [00:00, 12.9MB/s]                    \n",
      "2025-07-02 21:14:05 INFO: Downloaded file to C:\\Users\\hafid\\stanza_resources\\resources.json\n",
      "2025-07-02 21:14:05 INFO: Downloading default packages for language: en (English) ...\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.10.0/models/default.zip: 100%|██████████| 526M/526M [00:58<00:00, 9.06MB/s] \n",
      "2025-07-02 21:15:05 INFO: Downloaded file to C:\\Users\\hafid\\stanza_resources\\en\\default.zip\n",
      "2025-07-02 21:15:08 INFO: Finished downloading models and saved to C:\\Users\\hafid\\stanza_resources\n",
      "2025-07-02 21:15:08 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 432kB [00:00, 13.9MB/s]                    \n",
      "2025-07-02 21:15:08 INFO: Downloaded file to C:\\Users\\hafid\\stanza_resources\\resources.json\n",
      "2025-07-02 21:15:08 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-07-02 21:15:09 INFO: Loading these models for language: en (English):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | combined        |\n",
      "| mwt       | combined        |\n",
      "| pos       | combined_charlm |\n",
      "===============================\n",
      "\n",
      "2025-07-02 21:15:09 INFO: Using device: cpu\n",
      "2025-07-02 21:15:09 INFO: Loading: tokenize\n",
      "2025-07-02 21:15:09 INFO: Loading: mwt\n",
      "2025-07-02 21:15:09 INFO: Loading: pos\n",
      "2025-07-02 21:15:10 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hasil dari Stanza ---\n",
      "'Learning' -> Universal: NOUN, Detail: NN\n",
      "'POS' -> Universal: NOUN, Detail: NN\n",
      "'Tagging' -> Universal: NOUN, Detail: NN\n",
      "'is' -> Universal: AUX, Detail: VBZ\n",
      "'challenging' -> Universal: ADJ, Detail: JJ\n",
      "'for' -> Universal: ADP, Detail: IN\n",
      "'a' -> Universal: DET, Detail: DT\n",
      "'reason' -> Universal: NOUN, Detail: NN\n",
      "'.' -> Universal: PUNCT, Detail: .\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# Unduh model bahasa Inggris (hanya perlu sekali)\n",
    "stanza.download('en') \n",
    "\n",
    "# Buat pipeline NLP untuk bahasa Inggris, khusus untuk tokenisasi dan POS tagging\n",
    "# Ini akan memuat model ke memori\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos')\n",
    "\n",
    "teks = \"Learning POS Tagging is challenging for a reason.\"\n",
    "\n",
    "# Proses teks dengan pipeline\n",
    "doc = nlp(teks)\n",
    "\n",
    "print(\"\\n--- Hasil dari Stanza ---\")\n",
    "# Iterasi setiap kalimat, lalu setiap kata dalam dokumen\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        # .upos -> Tag Universal (Universal POS)\n",
    "        # .xpos -> Tag spesifik bahasa\n",
    "        print(f\"'{word.text}' -> Universal: {word.upos}, Detail: {word.xpos}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
